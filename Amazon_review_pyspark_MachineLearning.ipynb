{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bbchen33/Machine-Learning/blob/master/Amazon_review_pyspark_MachineLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu9mwhtbd2Ix",
        "colab_type": "text"
      },
      "source": [
        "Amazon cellphone reviews data from https://www.kaggle.com/grikomsn/amazon-cell-phones-reviews\n",
        "\n",
        "My goal is to use PySpark and machine learning to determine if it's possible to predict the ratings based on review contents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFGk2LWbdXR4",
        "colab_type": "code",
        "outputId": "55c1de2d-5bfa-4f0c-d361-3d272abd1106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "!unzip amazon-cell-phones-reviews.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  amazon-cell-phones-reviews.zip\n",
            "  inflating: 20191226-items.csv      \n",
            "  inflating: 20191226-reviews.csv    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjBX-KCiwIAi",
        "colab_type": "text"
      },
      "source": [
        "Use Java 8 and install pysark + spark-nlp  in colab https://github.com/JohnSnowLabs/spark-nlp#google-colab-notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbWpLGSjvxyV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "e0effb22-934b-4d4c-bbf3-383eb0c064ed"
      },
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed pyspark==2.4.4\n",
        "# Install Spark NLP\n",
        "! pip install --ignore-installed spark-nlp==2.4.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_242\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08)\n",
            "OpenJDK 64-Bit Server VM (build 25.242-b08, mixed mode)\n",
            "Collecting pyspark==2.4.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 215.7MB 61kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204kB 38.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130388 sha256=af33dc8a7c126826138718af55eff3673a253f2942c37cbbdb594c24814e5b3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.4\n",
            "Collecting spark-nlp==2.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/de/66e6e55931d41165d73c68636781dd60eb1a66e3bbe115d84fd67a541e26/spark_nlp-2.4.1-py2.py3-none-any.whl (108kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112kB 3.5MB/s \n",
            "\u001b[?25hInstalling collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64mla8NPv4OO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0d0b3180-68ac-4725-e9b7-c435c36c65cc"
      },
      "source": [
        "import sparknlp\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version\")\n",
        "sparknlp.version()\n",
        "print(\"Apache Spark version\")\n",
        "spark.version"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark NLP version\n",
            "Apache Spark version\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.4.4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue-75_fyN25A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local').appName('phone_reviews').getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM1_vTtIWYJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = spark.read.csv('20191226-reviews.csv', header = True, inferSchema = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUg0dDFDWhp8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "03cf985b-8ff5-4db6-aa8d-43fa7345ebac"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- asin: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- rating: integer (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- verified: boolean (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- body: string (nullable = true)\n",
            " |-- helpfulVotes: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X22QgIDtWmf2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "fdfaa2d4-8511-41f6-fe93-fdb028b3abc5"
      },
      "source": [
        "df.describe().show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------+--------------------+------------------+-----------------+------------------+--------------------+--------------------+\n",
            "|summary|      asin|                name|            rating|             date|             title|                body|        helpfulVotes|\n",
            "+-------+----------+--------------------+------------------+-----------------+------------------+--------------------+--------------------+\n",
            "|  count|     67986|               67986|             67986|            67986|             67986|               67974|               27826|\n",
            "|   mean|      null| 9.817021480769231E8|3.8079163357161767|             null|252.29166666666666|                 9.0|   7.538604467286025|\n",
            "| stddev|      null|2.5660842391329656E9|1.5829057573283758|             null| 612.3482542391192|                 NaN|  29.950618310021497|\n",
            "|    min|B0000SX2UC|  \"\"\"I am\"\" Bradley\"|                 1|    April 1, 2011|                 !| 4000mAh and Dual...| \"\" Hand Candy\"\"U...|\n",
            "|    max|B0825BB7SG|          ðŸ˜ºZutto ðŸ˜º|                 5|September 9, 2019|                ðŸ¥´|              ðŸ¥°ðŸ¥°ðŸ¥°|                time|\n",
            "+-------+----------+--------------------+------------------+-----------------+------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7v4e4pjet8v",
        "colab_type": "text"
      },
      "source": [
        "By using describe(), one can see that some of the review body is missing with 67974 entries rather than 67986 like the most of the columns. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGLWbkAGnDWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_df = df.select('rating', 'body')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydMlX6EZfIEe",
        "colab_type": "text"
      },
      "source": [
        "We can filter out rows where 'body' is null."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_47Kj47XFMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_df = new_df.filter(df.body.isNotNull())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5QxYp5NfRpb",
        "colab_type": "text"
      },
      "source": [
        "Now we can process the text in the body for machine learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lca1CZIfPocT",
        "colab_type": "code",
        "outputId": "fb0fcc1e-94bd-4eee-95e2-27f303d52a38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "new_df.columns"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rating', 'body']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WXyQ9I7YdUk",
        "colab_type": "text"
      },
      "source": [
        "Using Spark-nlp library to process text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYYwCckP19Lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.base import *\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol('body')\n",
        "\n",
        "sentence_detector = SentenceDetector() \\\n",
        "    .setInputCols(['document']) \\\n",
        "    .setOutputCol('sentence')\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols(['sentence']) \\\n",
        "    .setOutputCol('token')\n",
        "\n",
        "stemmer = Stemmer() \\\n",
        "    .setInputCols(['token']) \\\n",
        "    .setOutputCol('stem_token')\n",
        "normalizer = Normalizer() \\\n",
        "    .setInputCols(['stem_token']) \\\n",
        "    .setOutputCol('normalized')\n",
        "\n",
        "finisher = Finisher() \\\n",
        "    .setInputCols(['normalized']) \\\n",
        "    .setOutputCols(['ntokens']) \\\n",
        "    .setOutputAsArray(True) \\\n",
        "    .setCleanAnnotations(True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJMo3tZh4bPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_pipeline = Pipeline(stages = [document_assembler, \n",
        "                                  sentence_detector, \n",
        "                                  tokenizer, \n",
        "                                  stemmer, \n",
        "                                  normalizer, \n",
        "                                  finisher])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k0qiyZF5jSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_df = nlp_pipeline.fit(new_df).transform(new_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPOstmmZ52rT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "0d10bf96-3f45-4836-d6ef-df441631fd46"
      },
      "source": [
        "processed_df.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+--------------------+--------------------+\n",
            "|rating|                body|             ntokens|\n",
            "+------+--------------------+--------------------+\n",
            "|     3|I had the Samsung...|[i, had, the, sam...|\n",
            "|     1|\"Due to a softwar...|[due, to, a, soft...|\n",
            "|     5|This is a great, ...|[thi, i, a, great...|\n",
            "|     3|I love the phone ...|[i, love, the, ph...|\n",
            "|     4|The phone has bee...|[the, phone, ha, ...|\n",
            "|     4|Hello, I have thi...|[hello, i, have, ...|\n",
            "|     5|Cool. Cheap. Colo...|[cool, cheap, col...|\n",
            "|     4|The 3599i is over...|[the, i, i, overa...|\n",
            "|     5|I've never owned ...|[iv, never, own, ...|\n",
            "|     3|ok well im in sch...|[ok, well, im, in...|\n",
            "|     4|I've had this pho...|[iv, had, thi, ph...|\n",
            "|     1|1.) Slow - If you...|[slow, if, you, w...|\n",
            "|     2|I bought this pho...|[i, bought, thi, ...|\n",
            "|     4|This is an excell...|[thi, i, an, exce...|\n",
            "|     1|DON'T BUY OUT OF ...|[dont, bui, out, ...|\n",
            "|     4|I have been with ...|[i, have, been, w...|\n",
            "|     5|I just got it and...|[i, just, got, it...|\n",
            "|     1|1 star because th...|[star, becaus, th...|\n",
            "|     5|The product has b...|[the, product, ha...|\n",
            "|     1|\"My problems with...|[my, problem, wit...|\n",
            "+------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-njvgRAIwMD",
        "colab_type": "text"
      },
      "source": [
        "Additional text processing using Spark ml feature library. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HSsf1CUxSVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
        "\n",
        "stopwords = StopWordsRemover.loadDefaultStopWords('english')\n",
        "sw_remover = StopWordsRemover(inputCol = 'ntokens', outputCol = 'clean_tokens', stopWords = stopwords)\n",
        "cv = CountVectorizer(inputCol = 'clean_tokens', outputCol = 'TF', vocabSize = 500)\n",
        "idf = IDF(inputCol = 'TF', outputCol = 'IDF')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KRWw3ri1Gf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_pipeline2 = Pipeline(stages = [sw_remover, cv, idf])\n",
        "processed_df2 = nlp_pipeline2.fit(processed_df).transform(processed_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKTY5EqA1Gkb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "outputId": "53d73ae2-7912-41aa-8af7-121fc8f8e4e2"
      },
      "source": [
        "processed_df2.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|rating|                body|             ntokens|        clean_tokens|                  TF|                 IDF|\n",
            "+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     3|I had the Samsung...|[i, had, the, sam...|[samsung, awhil, ...|(500,[0,1,2,3,4,5...|(500,[0,1,2,3,4,5...|\n",
            "|     1|\"Due to a softwar...|[due, to, a, soft...|[due, softwar, is...|(500,[0,1,3,13,17...|(500,[0,1,3,13,17...|\n",
            "|     5|This is a great, ...|[thi, i, a, great...|[thi, great, reli...|(500,[0,1,5,8,10,...|(500,[0,1,5,8,10,...|\n",
            "|     3|I love the phone ...|[i, love, the, ph...|[love, phone, bec...|(500,[0,14,17,19,...|(500,[0,14,17,19,...|\n",
            "|     4|The phone has bee...|[the, phone, ha, ...|[phone, ha, great...|(500,[0,2,5,11,12...|(500,[0,2,5,11,12...|\n",
            "|     4|Hello, I have thi...|[hello, i, have, ...|[hello, thi, phon...|(500,[0,1,2,4,5,6...|(500,[0,1,2,4,5,6...|\n",
            "|     5|Cool. Cheap. Colo...|[cool, cheap, col...|[cool, cheap, col...|(500,[0,1,6,11,12...|(500,[0,1,6,11,12...|\n",
            "|     4|The 3599i is over...|[the, i, i, overa...|[overal, nice, ph...|(500,[0,2,19,29,3...|(500,[0,2,19,29,3...|\n",
            "|     5|I've never owned ...|[iv, never, own, ...|[iv, never, nokia...|(500,[0,1,5,10,27...|(500,[0,1,5,10,27...|\n",
            "|     3|ok well im in sch...|[ok, well, im, in...|[ok, well, im, sc...|(500,[0,1,2,3,11,...|(500,[0,1,2,3,11,...|\n",
            "|     4|I've had this pho...|[iv, had, thi, ph...|[iv, thi, phone, ...|(500,[0,1,3,4,5,6...|(500,[0,1,3,4,5,6...|\n",
            "|     1|1.) Slow - If you...|[slow, if, you, w...|[slow, want, chec...|(500,[0,1,2,3,4,7...|(500,[0,1,2,3,4,7...|\n",
            "|     2|I bought this pho...|[i, bought, thi, ...|[bought, thi, pho...|(500,[0,1,2,3,4,5...|(500,[0,1,2,3,4,5...|\n",
            "|     4|This is an excell...|[thi, i, an, exce...|[thi, excel, choi...|(500,[0,1,4,11,12...|(500,[0,1,4,11,12...|\n",
            "|     1|DON'T BUY OUT OF ...|[dont, bui, out, ...| [dont, bui, servic]|(500,[28,42,102],...|(500,[28,42,102],...|\n",
            "|     4|I have been with ...|[i, have, been, w...|[nextel, nearli, ...|(500,[0,1,13,17,1...|(500,[0,1,13,17,1...|\n",
            "|     5|I just got it and...|[i, just, got, it...|[got, sai, easi, ...|(500,[4,34,37,89,...|(500,[4,34,37,89,...|\n",
            "|     1|1 star because th...|[star, becaus, th...|[star, becaus, ph...|(500,[0,35,59,189...|(500,[0,35,59,189...|\n",
            "|     5|The product has b...|[the, product, ha...|[product, ha, ver...|(500,[0,1,3,4,6,8...|(500,[0,1,3,4,6,8...|\n",
            "|     1|\"My problems with...|[my, problem, wit...|[problem, nextel,...|(500,[0,1,2,3,4,8...|(500,[0,1,2,3,4,8...|\n",
            "+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBiE1WI1C2Ke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_df = processed_df2.select(['rating', 'IDF'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tfdq98S9Yc7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df, test_df = final_df.randomSplit(weights = [0.7, 0.3], seed = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1aBRVZaHHXn",
        "colab_type": "text"
      },
      "source": [
        "Using a simple logistic regression model for the first prediction to see how well it can do."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCVH431JP1Zk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression(featuresCol = 'IDF', labelCol = 'rating', maxIter = 10)\n",
        "lrModel = lr.fit(train_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNLBX9l0UcmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainingSummary = lrModel.summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwpoBAVQVCtR",
        "colab_type": "code",
        "outputId": "b7147ccc-6839-45ba-b14b-f8ea19b9dd3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trainingSummary.accuracy"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6805068921385561"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F7X0a1YW9sB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = lrModel.transform(test_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_mrojKMXBG_",
        "colab_type": "code",
        "outputId": "e8a7daf5-9e70-43dd-8ef6-184e161c4b3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions.columns"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rating', 'IDF', 'rawPrediction', 'probability', 'prediction']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxiHCItNXqEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol = 'rating', predictionCol = 'prediction', metricName = 'accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWdafXE9X8Hc",
        "colab_type": "code",
        "outputId": "b09443fd-cb73-4a87-f5b6-cae0c8d04658",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "evaluator.evaluate(predictions)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6690955639801093"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7k2x6IVYCSL",
        "colab_type": "text"
      },
      "source": [
        "The accuracy for the test set is 0.67. It's not amazing but given how simple the model is, 68% accuracy isn't bad. Things that can be done to improve the accuracy score: \n",
        "1. Word embedding instead of TF-IDF. Word embedding cares more about the context while TF-IDF only cares about frequency of the words. \n",
        "2. Sentiment analysis since the rating is likely to depend on the sentiment instead of the specific words used in the review. \n",
        "3. A more complex machine learning model. In my experience, SVC tends to work well with vectorized texts. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLSr1JOBQymR",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QjsGcU0MaBv",
        "colab_type": "text"
      },
      "source": [
        "Trying word2vec embedding instead of TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGTjAcSCKNxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import StopWordsRemover, Word2Vec, VectorAssembler\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
        "\n",
        "\n",
        "stopwords = StopWordsRemover.loadDefaultStopWords('english')\n",
        "sw_remover = StopWordsRemover(inputCol = 'ntokens', outputCol = 'clean_tokens', stopWords = stopwords)\n",
        "\n",
        "word2vec = Word2Vec(vectorSize = 100, minCount = 2, seed = 1,\n",
        "                    inputCol = 'clean_tokens', outputCol = 'embedding')\n",
        "assembler = VectorAssembler(inputCols = ['embedding'], outputCol = 'feature')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoDDI22pKN03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_pipeline3 = Pipeline(stages = [sw_remover, word2vec, assembler])\n",
        "processed_df3 = nlp_pipeline3.fit(processed_df).transform(processed_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8QsK0dSMZCU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "outputId": "6d52a826-aca4-4cc2-f741-8fb0bc536ed1"
      },
      "source": [
        "processed_df3.show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|rating|                body|             ntokens|        clean_tokens|           embedding|             feature|\n",
            "+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|     3|I had the Samsung...|[i, had, the, sam...|[samsung, awhil, ...|[-0.0635263526240...|[-0.0635263526240...|\n",
            "|     1|\"Due to a softwar...|[due, to, a, soft...|[due, softwar, is...|[-0.0084085425062...|[-0.0084085425062...|\n",
            "|     5|This is a great, ...|[thi, i, a, great...|[thi, great, reli...|[-0.0545550114236...|[-0.0545550114236...|\n",
            "|     3|I love the phone ...|[i, love, the, ph...|[love, phone, bec...|[-0.0973284343384...|[-0.0973284343384...|\n",
            "|     4|The phone has bee...|[the, phone, ha, ...|[phone, ha, great...|[-0.1161695932653...|[-0.1161695932653...|\n",
            "|     4|Hello, I have thi...|[hello, i, have, ...|[hello, thi, phon...|[-0.0121190463368...|[-0.0121190463368...|\n",
            "|     5|Cool. Cheap. Colo...|[cool, cheap, col...|[cool, cheap, col...|[-0.0676411662340...|[-0.0676411662340...|\n",
            "|     4|The 3599i is over...|[the, i, i, overa...|[overal, nice, ph...|[-0.0302828559943...|[-0.0302828559943...|\n",
            "|     5|I've never owned ...|[iv, never, own, ...|[iv, never, nokia...|[-0.1364813663450...|[-0.1364813663450...|\n",
            "|     3|ok well im in sch...|[ok, well, im, in...|[ok, well, im, sc...|[0.00102907404343...|[0.00102907404343...|\n",
            "|     4|I've had this pho...|[iv, had, thi, ph...|[iv, thi, phone, ...|[-0.0823522292861...|[-0.0823522292861...|\n",
            "|     1|1.) Slow - If you...|[slow, if, you, w...|[slow, want, chec...|[-0.0714677691142...|[-0.0714677691142...|\n",
            "|     2|I bought this pho...|[i, bought, thi, ...|[bought, thi, pho...|[-0.1035929976238...|[-0.1035929976238...|\n",
            "|     4|This is an excell...|[thi, i, an, exce...|[thi, excel, choi...|[-0.0922753920778...|[-0.0922753920778...|\n",
            "|     1|DON'T BUY OUT OF ...|[dont, bui, out, ...| [dont, bui, servic]|[0.09463293179093...|[0.09463293179093...|\n",
            "|     4|I have been with ...|[i, have, been, w...|[nextel, nearli, ...|[-0.0370393598399...|[-0.0370393598399...|\n",
            "|     5|I just got it and...|[i, just, got, it...|[got, sai, easi, ...|[0.04207117292522...|[0.04207117292522...|\n",
            "|     1|1 star because th...|[star, becaus, th...|[star, becaus, ph...|[0.00907299132086...|[0.00907299132086...|\n",
            "|     5|The product has b...|[the, product, ha...|[product, ha, ver...|[-0.0627656561486...|[-0.0627656561486...|\n",
            "|     1|\"My problems with...|[my, problem, wit...|[problem, nextel,...|[-0.0092480508738...|[-0.0092480508738...|\n",
            "+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScSBp2N_PQxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_df2 = processed_df3.select(['rating', 'feature'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knbylJ2dPXjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df2, test_df2 = processed_df3.randomSplit([0.7, 0.3], seed = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T519wyNUPdZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = LogisticRegression(featuresCol = 'feature', labelCol = 'rating', maxIter = 10)\n",
        "lrModel = lr.fit(train_df2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VyG5lbCPjSs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "959416f4-3ded-475f-f230-90381ba965f7"
      },
      "source": [
        "trainingSummary = lrModel.summary\n",
        "print(trainingSummary.accuracy)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.67299582485366\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve3xwRoIPpP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = lrModel.transform(test_df2)\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol = 'rating', predictionCol = 'prediction', metricName = 'accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ro3P_pTPwP2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b896f5eb-424a-4c4d-eb17-6438f7ebc94a"
      },
      "source": [
        "evaluator.evaluate(predictions)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6690463295751071"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NNtcIesbE74",
        "colab_type": "text"
      },
      "source": [
        "Just want to point out that word2vec didn't work better here but also the only input for embedding training is the text in this file. It might work better using pre-trained model. "
      ]
    }
  ]
}